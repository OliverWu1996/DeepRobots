{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries and Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Robots.DiscreteDeepRobots import ThreeLinkRobot\n",
    "from math import pi, log\n",
    "import os\n",
    "from csv_generator import generate_csv\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "%matplotlib notebook\n",
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning and Auxiliary Functinos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qlearner(num_robots, alpha, gamma, epsilon, theta_lower, theta_upper, a1_lower, a1_upper, \n",
    "             a2_lower, a2_upper, a_lower, a_upper, angle_interval, graphs = False, live_graphs = False):\n",
    "    \"\"\"\n",
    "    :param num_robots: number of robots\n",
    "    :param alpha: learning rate\n",
    "    :param gamma: discount rate\n",
    "    :param epsilon: probability of choosing random action while learning\n",
    "    :param theta_lower: lower limit of theta in state space\n",
    "    :param theta_upper: upper limit of theta in state space\n",
    "    :param a1_lower: lower limit of a1 in state space\n",
    "    :param a1_upper: upper limit of a1 in state space\n",
    "    :param a2_lower: lower limit of a2 in state space\n",
    "    :param a2_upper: upper limit of a2 in state space\n",
    "    :param a_lower: lower limit of action space\n",
    "    :param a_upper: upper limit of action space\n",
    "    :param angle_interval: interval of state and action space\n",
    "    :param graphs: whether to generate graphs\n",
    "    :param live_graphs: whether to generate dynamics graphs\n",
    "    :return: a tuple of learned q-values, states, actions\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize state space, action space, and Qvalues\n",
    "    print('loading state space')\n",
    "    states = get_state_space(theta_lower, theta_upper,a1_lower,\n",
    "                             a1_upper, a2_lower, a2_upper, angle_interval)  # state = (theta, a1, a2)\n",
    "    print(len(states), 'states loaded')\n",
    "    print('loading action space')\n",
    "    actions = get_action_space(a_lower, a_upper, angle_interval)  # action = (a1dot, a2dot)\n",
    "    print(len(actions), 'actions loaded')\n",
    "    Qvalues = {}\n",
    "    print('initializing Qvalues')\n",
    "    for state in states:\n",
    "        for action in actions:\n",
    "            Qvalues[(state, action)] = 0\n",
    "    print(len(Qvalues.keys()), 'q-values loaded')\n",
    "\n",
    "    # learn q-values\n",
    "    print('\\n\\n')\n",
    "    details = {}\n",
    "    \n",
    "    for j in range(num_robots):\n",
    "        print(j+1, ' ith ', 'robot is learning')\n",
    "        # state = random.choice(states)\n",
    "#         robot = ThreeLinkRobot(x=0, \n",
    "#                                y=0, \n",
    "#                                theta=state[0], \n",
    "#                                a1=state[1],\n",
    "#                                a2=state[2],\n",
    "#                                link_length=2, \n",
    "#                                t_interval=1, \n",
    "#                                a_interval=angle_interval)\n",
    "        robot = ThreeLinkRobot(x=0, \n",
    "                               y=0, \n",
    "                               theta=0, \n",
    "                               a1=pi/16,\n",
    "                               a2=-pi/16,\n",
    "                               link_length=2, \n",
    "                               t_interval=1, \n",
    "                               a_interval=angle_interval)\n",
    "        i = 0\n",
    "        \n",
    "        # plotting\n",
    "        xs = [robot.x]\n",
    "        a1s = [robot.a1]\n",
    "        a2s = [robot.a2]\n",
    "        timesteps = [i]\n",
    "        \n",
    "        for k in range(100):\n",
    "            i += 1\n",
    "            old_x = robot.x\n",
    "            # print('For', j+1, 'th robot', 'In', i, 'th iteration the initial state is: ', state)\n",
    "            # employ an epsilon-greedy strategy for exploration vs exploitation\n",
    "            \n",
    "            best_actions = []\n",
    "            if random.random() < epsilon:\n",
    "\n",
    "                # choose a random action\n",
    "                best_actions = actions\n",
    "                # randomly select a tie-breaking, valid action\n",
    "                while True:\n",
    "                    best_action = random.choice(best_actions)\n",
    "                    # print('The action randomly chosen is', best_action)\n",
    "                    temp_robot = copy.deepcopy(robot)\n",
    "                    temp_robot.move(best_action[0], best_action[1], 1)\n",
    "                    if (temp_robot.theta, temp_robot.a1, temp_robot.a2) in states:\n",
    "                        break\n",
    "            else:\n",
    "\n",
    "                # find the best actions (the ones with largest q-value)\n",
    "                maxQ = -float(\"inf\")\n",
    "                for action in actions:\n",
    "                    Q = Qvalues[(state, action)]\n",
    "                    \n",
    "                    # identify action with higher Q value\n",
    "                    temp_robot = copy.deepcopy(robot)\n",
    "                    if Q > maxQ:\n",
    "                        \n",
    "                        # check if action is valid\n",
    "                        temp_robot.move(action[0], action[1], 1)\n",
    "                        if (temp_robot.theta, temp_robot.a1, temp_robot.a2) in states:\n",
    "                            best_actions = [action]\n",
    "                            maxQ = Q\n",
    "                    elif Q == maxQ:\n",
    "                        temp_robot.move(action[0], action[1], 1)\n",
    "                        if (temp_robot.theta, temp_robot.a1, temp_robot.a2) in states:\n",
    "                            best_actions.append(action)\n",
    "                    del temp_robot\n",
    "                \n",
    "                # randomly select a tie-breaking, valid action\n",
    "                best_action = random.choice(best_actions)\n",
    "\n",
    "\n",
    "            # print('In', i, 'th iteration the best action is: ', best_action)\n",
    "            robot.move(best_action[0],best_action[1],1)\n",
    "\n",
    "            # transition to new state\n",
    "            new_state = robot.state\n",
    "            new_x = robot.x\n",
    "            # print('In', i, 'th iteration the new state is: ', new_state)\n",
    "            \n",
    "            # add values to lists\n",
    "            xs.append(robot.x)\n",
    "            a1s.append(robot.a1)\n",
    "            a2s.append(robot.a2)\n",
    "            timesteps.append(i)\n",
    "\n",
    "            # plotting\n",
    "            if live_graphs:\n",
    "                make_graphs(xs,a1s,a2s,timesteps,j+1,live=True, Q=True)\n",
    "            \n",
    "\n",
    "            # find the maximum Q value for new state\n",
    "            Q = -float(\"inf\")\n",
    "            for action in actions:\n",
    "                Q = max(Q, Qvalues[(new_state, action)])\n",
    "\n",
    "            # find the reward of this transition\n",
    "            # reward = 0\n",
    "            a1, a2, R, a1dot, a2dot = robot.a1, robot.a2, robot.R, robot.a1dot, robot.a2dot\n",
    "            \n",
    "            '''\n",
    "            # penalize according to joint proximity\n",
    "            if a1 == a2:\n",
    "                reward += -10*R\n",
    "            \n",
    "            else:\n",
    "                print('In ', i, 'th iteration the penalty for joint angle proximity is: ', log(a1-a2), 'for joint angles: ', a1, a2)\n",
    "                reward += log(a1-a2)\n",
    "            '''\n",
    "            \n",
    "            # print('In ', i, 'th iteration the reward for x- velocity is: ', v/(a1dot**2 + a2dot**2), 'for velocity, a1dot, a2dot: ', v, a1dot, a2dot)\n",
    "            # reward += v/(a1dot**2 + a2dot**2)\n",
    "            reward = 100*(new_x-old_x)\n",
    "\n",
    "            # TD update\n",
    "            sample = gamma * Q\n",
    "            old_Q = Qvalues[(state, best_action)]\n",
    "            # print('In ', i, 'th iteration the Q value before update is: ', old_Q)\n",
    "            new_Q = (1 - alpha) * old_Q + alpha * (reward + sample)\n",
    "            Qvalues[(state, best_action)] = new_Q\n",
    "            # print('In ', i, 'th iteration the Q value after update is: ', new_Q)\n",
    "            state = new_state\n",
    "\n",
    "#             # check for convergence\n",
    "#             if old_Q == 0:\n",
    "#                 pass\n",
    "#             elif abs((new_Q-old_Q)/old_Q) <= 0.05:\n",
    "#                 print('algorithm converged')\n",
    "#                 break\n",
    "        if graphs:\n",
    "            make_graphs(xs, a1s, a2s, timesteps, j,Q=True)\n",
    "        details[j] = (xs, a1s, a2s, timesteps)\n",
    "        \n",
    "    return Qvalues, states, actions, details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_action_space(lower_limit, upper_limit, interval):\n",
    "    \"\"\"\n",
    "    auxiliary function used by Qlearner() to get action space\n",
    "    :return: a list of action space values in tuple format (a1dot, a2dot)\n",
    "    \"\"\"\n",
    "    upper_limit += (interval/10)  # to ensure the range covers the rightmost value in the loop\n",
    "    r = np.arange(lower_limit, upper_limit, interval)\n",
    "    space = [(rnd(i), rnd(j)) for i in r for j in r]\n",
    "\n",
    "    # remove a1dot = 0, a2dot = 0 from action space\n",
    "    space.remove((0,0))\n",
    "\n",
    "    return space\n",
    "\n",
    "\n",
    "def get_state_space(theta_lower, theta_upper,\n",
    "                    a1_lower, a1_upper, a2_lower, a2_upper, angle_interval):\n",
    "    \"\"\"\n",
    "    auxiliary function used by Qlearner() to get action space\n",
    "    :return: a list of state space values in tuple format (a1, a2, theta)\n",
    "    \"\"\"\n",
    "    # to ensure the range covers the rightmost value in the loop\n",
    "    theta_upper += (angle_interval/10)\n",
    "    a1_upper += (angle_interval/10)\n",
    "    a2_upper += (angle_interval/10)\n",
    "\n",
    "    theta_range = np.arange(theta_lower, theta_upper, angle_interval)\n",
    "    a1_range = np.arange(a1_lower, a1_upper, angle_interval)\n",
    "    a2_range = np.arange(a2_lower, a2_upper, angle_interval)\n",
    "    space = [(rnd(theta), rnd(a1), rnd(a2)) for theta in theta_range for a1 in a1_range for a2 in a2_range]\n",
    "\n",
    "    return space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnd(number):\n",
    "    return round(number, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_graphs(xs, a1s, a2s, timesteps, number, PATH, robot_num, live=False, Q=False):\n",
    "\n",
    "    # plotting\n",
    "    fig1 = plt.figure(1)\n",
    "    if Q:\n",
    "        fig1.suptitle('Q-learning Monitor for robot ' + str(robot_num))\n",
    "    else:\n",
    "        fig1.suptitle('Policy Rollout')\n",
    "    ax1 = fig1.add_subplot(311)\n",
    "    ax2 = fig1.add_subplot(312)\n",
    "    ax3 = fig1.add_subplot(313)\n",
    "    # fig2 = plt.figure(2)\n",
    "    # fig2.suptitle('a1 vs a2')\n",
    "    # ax4 = fig2.add_subplot(111)\n",
    "\n",
    "    ax1.plot(steps, xs, '.-')\n",
    "    ax1.set_ylabel('x')\n",
    "    ax1.set_xlabel('steps')\n",
    "    ax2.plot(steps, a1s, '.-')\n",
    "    ax2.set_ylabel('a1')\n",
    "    ax2.set_xlabel('steps')\n",
    "    ax3.plot(steps, a2s, '.-')\n",
    "    ax3.set_ylabel('a2')\n",
    "    ax3.set_xlabel('steps')\n",
    "    # ax4.plot(a1s,a2s,'.-')\n",
    "    # ax4.set_xlabel('a1')\n",
    "    # ax4.set_ylabel('a2')\n",
    "\n",
    "    fig1.tight_layout()\n",
    "    # fig2.tight_layout()\n",
    "    fig1.tight_layout()\n",
    "    fig1.savefig(PATH + \"/\" + str(number) + ' th Policy Rollout.png')\n",
    "    plt.close(fig1)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy and Q-value Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_policy(Qvalues, states, actions):\n",
    "    policy = {}\n",
    "    for state in states:\n",
    "        maxQ = -float(\"inf\")\n",
    "        best_action = None\n",
    "        for action in actions:\n",
    "            Q = Qvalues[(state, action)]\n",
    "            maxQ = max(Q, maxQ)\n",
    "            if Q == maxQ:\n",
    "                best_action = action\n",
    "        policy[state] = best_action\n",
    "    return policy\n",
    "\n",
    "\n",
    "def test_policy(robot, policy, timestep=20):\n",
    "    robot_params = []\n",
    "    robot_param = [robot.x, robot.y, robot.theta, robot.a1, robot.a2, robot.a1dot, robot.a2dot]\n",
    "    robot_params.append(robot_param)\n",
    "    xs = [robot.x]\n",
    "    a1s = [robot.a1]\n",
    "    a2s = [robot.a2]\n",
    "    timesteps = [0]\n",
    "    plt.title('Policy Rollout')\n",
    "    for i in range(timestep):\n",
    "        \n",
    "        # rollout\n",
    "        initial_state = robot.state\n",
    "        print('In', i, 'th iteration the initial state is: ', initial_state)\n",
    "        old_x = robot.x\n",
    "        action = policy[initial_state]\n",
    "        print('In', i, 'th iteration the chosen action is: ', action)\n",
    "        robot.move(action[0], action[1], 1)\n",
    "        new_x = robot.x\n",
    "        print('In ', i, 'th iteration, the robot moved ', new_x - old_x, ' in x direction')\n",
    "        \n",
    "        # add values to lists\n",
    "        xs.append(robot.x)\n",
    "        a1s.append(robot.a1)\n",
    "        a2s.append(robot.a2)\n",
    "        timesteps.append(i+1)\n",
    "        \n",
    "        # plotting\n",
    "        make_graphs(dxs,a1s,a2s,timesteps,0,live=True)\n",
    "        \n",
    "        plt.plot(a1s,a2s,'.-')\n",
    "        plt.xlabel('a1')\n",
    "        plt.ylabel('a2')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    plt.show()\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading state space\n",
      "20 states loaded\n",
      "loading action space\n",
      "8 actions loaded\n",
      "initializing Qvalues\n",
      "160 q-values loaded\n",
      "\n",
      "\n",
      "\n",
      "1  ith  robot is learning\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-c7b81b946e0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mangle_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mgraphs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         live_graphs=False)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-6e77e904eeb1>\u001b[0m in \u001b[0;36mQlearner\u001b[0;34m(num_robots, alpha, gamma, epsilon, theta_lower, theta_upper, a1_lower, a1_upper, a2_lower, a2_upper, a_lower, a_upper, angle_interval, graphs, live_graphs)\u001b[0m\n\u001b[1;32m     82\u001b[0m                     \u001b[0;31m# print('The action randomly chosen is', best_action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mtemp_robot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrobot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mtemp_robot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtemp_robot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_robot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_robot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Projects/DeepRobots/Robots/DiscreteDeepRobots.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(self, a1dot, a2dot, timestep)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma1dot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma2dot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimestep\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_interval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperform_integration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;31m# testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Projects/DeepRobots/Robots/DiscreteDeepRobots.py\u001b[0m in \u001b[0;36mperform_integration\u001b[0;34m(self, action, t_interval)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mv0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0msol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0modeint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1dot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma2dot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msol\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/scipy/integrate/odepack.py\u001b[0m in \u001b[0;36modeint\u001b[0;34m(func, y0, t, args, Dfun, col_deriv, full_output, ml, mu, rtol, atol, tcrit, h0, hmax, hmin, ixpr, mxstep, mxhnil, mxordn, mxords, printmessg)\u001b[0m\n\u001b[1;32m    213\u001b[0m     output = _odepack.odeint(func, y0, t, args, Dfun, col_deriv, ml, mu,\n\u001b[1;32m    214\u001b[0m                              \u001b[0mfull_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtcrit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                              ixpr, mxstep, mxhnil, mxordn, mxords)\n\u001b[0m\u001b[1;32m    216\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mwarning_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_msgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Run with full_output = 1 to get quantitative information.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Projects/DeepRobots/Robots/DiscreteDeepRobots.py\u001b[0m in \u001b[0;36mrobot\u001b[0;34m(self, v, t, da1, da2)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mrobot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mda1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mda2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \"\"\"\n\u001b[1;32m    115\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0modeint\u001b[0m \u001b[0mintegration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# learn Q values\n",
    "Qvalues, states, actions, details = Qlearner(num_robots=300,\n",
    "        alpha=0.5,\n",
    "        gamma=0.9,\n",
    "        epsilon=0.5,\n",
    "        theta_lower=-pi/8,\n",
    "        theta_upper=pi/8,\n",
    "        a1_lower=pi/32,\n",
    "        a1_upper=pi/8,\n",
    "        a2_lower=-pi/8,\n",
    "        a2_upper=-pi/32,\n",
    "        a_lower=-pi/16,\n",
    "        a_upper=pi/16,\n",
    "        angle_interval=pi/16,\n",
    "        graphs=False,\n",
    "        live_graphs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Analysis of Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # graphing Q learning details\n",
    "# for k in details.keys():\n",
    "#     xs, a1s, a2s, timesteps = details[k]\n",
    "#     %matplotlib inline\n",
    "#     make_graphs(xs, a1s, a2s, timesteps, k+1, Q=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # graph Fourier transform\n",
    "# for k in details.keys():\n",
    "#     xs, a1s, a2s, timesteps = details[k]\n",
    "#     xs_ = np.fft.fft(xs)\n",
    "#     fig = plt.figure(1)\n",
    "#     ax1 = fig.add_subplot(311)\n",
    "#     ax1.plot(timesteps, xs_,'.-')\n",
    "\n",
    "#     a1s_ = np.fft.fft(a1s)\n",
    "#     ax2 = fig.add_subplot(312)\n",
    "#     ax2.plot(timesteps, a1s_,'.-')\n",
    "\n",
    "#     a2s_ = np.fft.fft(a2s)\n",
    "#     ax3 = fig.add_subplot(313)\n",
    "#     ax3.plot(timesteps, a2s_,'.-')\n",
    "#     # %matplotlib inline\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Politcy Rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract policy\n",
    "policy = extract_policy(Qvalues, states, actions)\n",
    "for state in policy:\n",
    "    print('state', rnd(state[0]),rnd(state[1]),rnd(state[2]), 'action', rnd(policy[state][0]),rnd(policy[state][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TIMESTAMP = str(datetime.datetime.now())\n",
    "TRIAL_NAME = ' DiscreteRL '\n",
    "TRIAL_NUM = 1\n",
    "PATH = 'Trials/' + TRIAL_NAME + 'Trial ' + str(TRIAL_NUM) + \" \" + TIMESTAMP\n",
    "os.mkdir(PATH)\n",
    "os.chmod(PATH, 0o0777)\n",
    "\n",
    "TIMESTEPS = 100\n",
    "for j in range(1):\n",
    "    robot = ThreeLinkRobot(x=0, \n",
    "                           y=0, \n",
    "                           theta=0, \n",
    "                           a1=pi/16,\n",
    "                           a2=-pi/16,\n",
    "                           link_length=2, \n",
    "                           t_interval=1, \n",
    "                           a_interval=pi/32)\n",
    "    xs = [robot.x]\n",
    "    a1s = [robot.a1]\n",
    "    a2s = [robot.a2]\n",
    "    steps = [0]\n",
    "    # robot.randomize_state(enforce_opposite_angle_signs=True)\n",
    "    robot_params = []\n",
    "    robot_param = [robot.x, robot.y, robot.theta, float(robot.a1), float(robot.a2), robot.a1dot, robot.a2dot]\n",
    "    robot_params.append(robot_param)\n",
    "    print('Beginning', j+1,  'th Policy Rollout')\n",
    "    try:\n",
    "        for i in range(TIMESTEPS):\n",
    "\n",
    "            # rollout\n",
    "            state = robot.state\n",
    "            print('In', i, 'th iteration the initial state is: ', state)\n",
    "            old_x = robot.x\n",
    "            action = policy[state]\n",
    "            print('In', i, 'th iteration the chosen action is: ', action)\n",
    "            robot.move(action[0], action[1], 1)\n",
    "            new_x = robot.x\n",
    "            print('In ', i, 'th iteration, the robot moved ', new_x - old_x, ' in x direction')\n",
    "\n",
    "            # add values to lists\n",
    "            xs.append(robot.x)\n",
    "            a1s.append(robot.a1)\n",
    "            a2s.append(robot.a2)\n",
    "            steps.append(i+1)\n",
    "            robot_param = [robot.x, robot.y, robot.theta, float(robot.a1), float(robot.a2), robot.a1dot, robot.a2dot]\n",
    "            robot_params.append(robot_param)\n",
    "\n",
    "    except ZeroDivisionError as e:\n",
    "        print(str(e), 'occured at ', j+1, 'th policy rollout')\n",
    "\n",
    "    # plotting\n",
    "    make_graphs(xs,a1s,a2s,steps,j+1,PATH,0)\n",
    "    generate_csv(robot_params, PATH + \"/\" + str(j+1) + \" th rollout.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
