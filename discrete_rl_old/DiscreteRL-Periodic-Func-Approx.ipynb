{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries and Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from DiscreteDeepRobots import ThreeLinkRobot\n",
    "from math import pi, log\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "%matplotlib notebook\n",
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning and Auxiliary Functinos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Qlearner(num_robots, alpha, gamma, epsilon, angle_range, angle_interval, graphs = False, live_graphs = False, Qvalues=None):\n",
    "    \"\"\"\n",
    "    :param num_robots: number of robots\n",
    "    :param alpha: learning rate\n",
    "    :param gamma: discount rate\n",
    "    :param epsilon: probability of choosing random action while learning\n",
    "    :param angle_range: (theta_lower,theta_upper,a1_lower,a1_upper,a2_lower,a2_upper,a_lower,a_upper)\n",
    "    :param angle_interval: interval of state and action space\n",
    "    :param graphs: whether to generate graphs\n",
    "    :param live_graphs: whether to generate dynamics graphs\n",
    "    :return: a tuple of learned q-values, states, actions\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize state space, action space, and Qvalues\n",
    "    print('Qlearner initiated')\n",
    "    print('loading state space')\n",
    "    states = get_state_space(angle_range, angle_interval)  # state = (theta, a1, a2)\n",
    "    print(len(states), 'states loaded')\n",
    "    print('loading action space')\n",
    "    actions = get_action_space(angle_range[6], angle_range[7], angle_interval)  # action = (a1dot, a2dot)\n",
    "    print(len(actions), 'actions loaded')\n",
    "    if Qvalues is None:\n",
    "        Qvalues = {}\n",
    "        print('initializing Qvalues')\n",
    "        for state in states:\n",
    "            for action in actions:\n",
    "                Qvalues[(state, action)] = 0\n",
    "        print(len(Qvalues.keys()), 'q-values loaded')\n",
    "    else:\n",
    "        print('Pretrained Qvalues loaded')\n",
    "\n",
    "    # learn q-values\n",
    "    print('\\n\\n')\n",
    "    details = {}\n",
    "    \n",
    "    for j in range(num_robots):\n",
    "        # print(j+1, ' ith ', 'robot is learning')\n",
    "        state = random.choice(states)\n",
    "        robot = ThreeLinkRobot(x=0, \n",
    "                               y=0, \n",
    "                               theta=state[0], \n",
    "                               a1=state[1],\n",
    "                               a2=state[2],\n",
    "                               link_length=2, \n",
    "                               t_interval=0.01, \n",
    "                               a_interval=angle_interval)\n",
    "        i = 0\n",
    "        \n",
    "        # plotting\n",
    "        xs = [robot.x]\n",
    "        a1s = [robot.a1]\n",
    "        a2s = [robot.a2]\n",
    "        timesteps = [i]\n",
    "        \n",
    "        while True:\n",
    "            i += 1\n",
    "            # print('For', j, 'th robot', 'In', i, 'th iteration the initial state is: ', state)\n",
    "            # employ an epsilon-greedy strategy for exploration vs exploitation\n",
    "            best_actions = []\n",
    "            if random.random() < epsilon:\n",
    "\n",
    "                # choose a random action\n",
    "                best_actions = actions\n",
    "                # randomly select a tie-breaking, valid action\n",
    "                while True:\n",
    "                    best_action = random.choice(best_actions)\n",
    "                    # print('The action randomly chosen is', best_action)\n",
    "                    temp_robot = copy.deepcopy(robot)\n",
    "                    temp_robot.move(best_action[0], best_action[1], 1)\n",
    "                    if (temp_robot.theta, temp_robot.a1, temp_robot.a2) in states:\n",
    "                        break\n",
    "            else:\n",
    "\n",
    "                # find the best actions (the ones with largest q-value)\n",
    "                maxQ = -float(\"inf\")\n",
    "                for action in actions:\n",
    "                    Q = Qvalues[(state, action)]\n",
    "                    \n",
    "                    # identify action with higher Q value\n",
    "                    temp_robot = copy.deepcopy(robot)\n",
    "                    if Q > maxQ:\n",
    "                        \n",
    "                        # check if action is valid\n",
    "                        temp_robot.move(action[0], action[1], 1)\n",
    "                        if (temp_robot.theta, temp_robot.a1, temp_robot.a2) in states:\n",
    "                            best_actions = [action]\n",
    "                            maxQ = Q\n",
    "                    elif Q == maxQ:\n",
    "                        temp_robot.move(action[0], action[1], 1)\n",
    "                        if (temp_robot.theta, temp_robot.a1, temp_robot.a2) in states:\n",
    "                            best_actions.append(action)\n",
    "                    del temp_robot\n",
    "                \n",
    "                # randomly select a tie-breaking, valid action\n",
    "                best_action = random.choice(best_actions)\n",
    "\n",
    "\n",
    "            # print('In', i, 'th iteration the best action is: ', best_action)\n",
    "            robot.move(best_action[0],best_action[1],1)\n",
    "\n",
    "            # transition to new state\n",
    "            new_state = robot.state\n",
    "            # print('In', i, 'th iteration the new state is: ', new_state)\n",
    "            \n",
    "            # add values to lists\n",
    "            xs.append(robot.x)\n",
    "            a1s.append(robot.a1)\n",
    "            a2s.append(robot.a2)\n",
    "            timesteps.append(i)\n",
    "\n",
    "            # plotting\n",
    "            if live_graphs:\n",
    "                make_graphs(xs,a1s,a2s,timesteps,j+1,live=True, Q=True)    \n",
    "\n",
    "            # find the maximum Q value for new state\n",
    "            Q = -float(\"inf\")\n",
    "            for action in actions:\n",
    "                Q = max(Q, Qvalues[(new_state, action)])\n",
    "\n",
    "            # find the reward of this transition\n",
    "            reward = 0\n",
    "            a1, a2, R, v, a1dot, a2dot = robot.a1, robot.a2, robot.R, robot.body_v[0], robot.a1dot, robot.a2dot\n",
    "            \n",
    "            '''\n",
    "            # penalize according to joint proximity\n",
    "            if a1 == a2:\n",
    "                reward += -10*R\n",
    "            \n",
    "            else:\n",
    "                print('In ', i, 'th iteration the penalty for joint angle proximity is: ', log(a1-a2), 'for joint angles: ', a1, a2)\n",
    "                reward += log(a1-a2)\n",
    "            '''\n",
    "            \n",
    "            # print('In ', i, 'th iteration the reward for x- velocity is: ', v/(a1dot**2 + a2dot**2), 'for velocity, a1dot, a2dot: ', v, a1dot, a2dot)\n",
    "            reward += v/(a1dot**2 + a2dot**2)\n",
    "\n",
    "            # TD update\n",
    "            sample = gamma * Q\n",
    "            old_Q = Qvalues[(state, best_action)]\n",
    "            # print('In ', i, 'th iteration the Q value before update is: ', old_Q)\n",
    "            new_Q = (1 - alpha) * old_Q + alpha * (reward + sample)\n",
    "            Qvalues[(state, best_action)] = new_Q\n",
    "            # print('In ', i, 'th iteration the Q value after update is: ', new_Q)\n",
    "            state = new_state\n",
    "\n",
    "            # check for convergence\n",
    "            if old_Q == 0:\n",
    "                pass\n",
    "            elif abs((new_Q-old_Q)/old_Q) <= 0.05:\n",
    "                print(j+1, ' th ', 'robot RL converged')\n",
    "                break\n",
    "        if graphs:\n",
    "            make_graphs(xs, a1s, a2s, timesteps, j,Q=True)\n",
    "        details[j] = (xs, a1s, a2s, timesteps)\n",
    "        \n",
    "    return Qvalues, states, actions, details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_action_space(lower_limit, upper_limit, interval):\n",
    "    \"\"\"\n",
    "    auxiliary function used by Qlearner() to get action space\n",
    "    :return: a list of action space values in tuple format (a1dot, a2dot)\n",
    "    \"\"\"\n",
    "    upper_limit += (interval/10)  # to ensure the range covers the rightmost value in the loop\n",
    "    r = np.arange(lower_limit, upper_limit, interval)\n",
    "    space = [(rnd(i), rnd(j)) for i in r for j in r]\n",
    "\n",
    "    # remove a1dot = 0, a2dot = 0 from action space\n",
    "    space.remove((0,0))\n",
    "\n",
    "    return space\n",
    "\n",
    "\n",
    "def get_state_space(angle_range, angle_interval):\n",
    "    \"\"\"\n",
    "    auxiliary function used by Qlearner() to get action space\n",
    "    :param angle_range: (theta_lower,theta_upper,a1_lower,a1_upper,a2_lower,a2_upper,a_lower,a_upper)\n",
    "    :return: a list of state space values in tuple format (a1, a2, theta)\n",
    "    \"\"\"\n",
    "    theta_lower,theta_upper,a1_lower,a1_upper,a2_lower,a2_upper,a_lower,a_upper = angle_range\n",
    "    \n",
    "    # to ensure the range covers the rightmost value in the loop\n",
    "    theta_upper += (angle_interval/10)\n",
    "    a1_upper += (angle_interval/10)\n",
    "    a2_upper += (angle_interval/10)\n",
    "\n",
    "    theta_range = np.arange(theta_lower, theta_upper, angle_interval)\n",
    "    a1_range = np.arange(a1_lower, a1_upper, angle_interval)\n",
    "    a2_range = np.arange(a2_lower, a2_upper, angle_interval)\n",
    "    space = [(rnd(theta), rnd(a1), rnd(a2)) for theta in theta_range for a1 in a1_range for a2 in a2_range]\n",
    "\n",
    "    return space\n",
    "\n",
    "def rnd(number):\n",
    "    return round(number, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_graphs(xs, a1s, a2s, timesteps, robot_num, live=False, Q=False):\n",
    "    # plotting\n",
    "    fig1 = plt.figure(1)\n",
    "    if Q:\n",
    "        fig1.suptitle('Q-learning Monitor for robot ' + str(robot_num))\n",
    "    else:\n",
    "        fig1.suptitle('Policy Rollout')\n",
    "    ax1 = fig1.add_subplot(311)\n",
    "    ax2 = fig1.add_subplot(312)\n",
    "    ax3 = fig1.add_subplot(313)\n",
    "    fig2 = plt.figure(2)\n",
    "    ax4 = fig2.add_subplot(111, projection='3d')\n",
    "    \n",
    "    ax1.plot(timesteps, xs, '.-')\n",
    "    ax1.set_ylabel('x')\n",
    "    ax2.plot(timesteps, a1s, '.-')\n",
    "    ax2.set_ylabel('a1')\n",
    "    ax3.plot(timesteps, a2s, '.-')\n",
    "    ax3.set_ylabel('a2')\n",
    "    \n",
    "    ax4.set_title('3D Monitor for a1, a2 vs time')\n",
    "    ax4.plot(timesteps, a1s, a2s, '.-')\n",
    "    fig1.tight_layout()\n",
    "    fig2.tight_layout()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy and Q-value Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_policy(model, states, actions, Qvalues = None):\n",
    "    policy = {}\n",
    "    if model is not None:\n",
    "        for state in states:\n",
    "            maxQ = -float(\"inf\")\n",
    "            best_action = None\n",
    "            for action in actions:\n",
    "                feature = [val for val in state]\n",
    "                for val in action: \n",
    "                    feature.append(val)\n",
    "                Q = model.predict(np.array([np.array(feature)]))\n",
    "                maxQ = max(Q, maxQ)\n",
    "                if Q == maxQ:\n",
    "                    best_action = action\n",
    "            policy[state] = best_action\n",
    "    else:\n",
    "        for state in states:\n",
    "            maxQ = -float(\"inf\")\n",
    "            best_action = None\n",
    "            for action in actions:\n",
    "                Q = Qvalues[(state, action)]\n",
    "                maxQ = max(Q, maxQ)\n",
    "                if Q == maxQ:\n",
    "                    best_action = action\n",
    "            policy[state] = best_action\n",
    "    return policy\n",
    "\n",
    "\n",
    "def test_policy(robot, policy, timestep=20):\n",
    "    dx = 0\n",
    "    dxs = []\n",
    "    a1s = []\n",
    "    a2s = []\n",
    "    timesteps = []\n",
    "    plt.title('Policy Rollout')\n",
    "    for i in range(timestep):\n",
    "        \n",
    "        # rollout\n",
    "        initial_state = robot.state\n",
    "        print('In', i+1, 'th iteration the initial state is: ', initial_state)\n",
    "        old_x = robot.x\n",
    "        action = policy[initial_state]\n",
    "        print('In', i+1, 'th iteration the chosen action is: ', action)\n",
    "        robot.move(action[0], action[1], 1)\n",
    "        new_x = robot.x\n",
    "        print('In ', i+1, 'th iteration, the robot moved ', new_x - old_x, ' in x direction')\n",
    "        dx += (new_x-old_x)\n",
    "        \n",
    "        # add values to lists\n",
    "        dxs.append(dx)\n",
    "        a1s.append(robot.a1)\n",
    "        a2s.append(robot.a2)\n",
    "        timesteps.append(i)\n",
    "        \n",
    "        # plotting\n",
    "        make_graphs(dxs,a1s,a2s,timesteps,0,live=True)\n",
    "        \n",
    "        plt.plot(a1s,a2s,'.-')\n",
    "        plt.xlabel('a1')\n",
    "        plt.ylabel('a2')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    plt.show()\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional Approximation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_training_inputs(Qvalues):\n",
    "    X, Y = [], []\n",
    "    for key in Qvalues:\n",
    "        if Qvalues[key] == 0:\n",
    "            continue\n",
    "        else:\n",
    "            feature = []\n",
    "            for element in key:\n",
    "                for val in element:\n",
    "                    feature.append(val)\n",
    "            X.append(np.array(feature))\n",
    "            Y.append(Qvalues[key])\n",
    "    X, Y = np.array(X),np.array(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fill in null Q values with Random Forest Predictions\n",
    "def fill_Q(Qvalues, model):\n",
    "    i = 1\n",
    "    for key in Qvalues:\n",
    "        if Qvalues[key] != 0:\n",
    "            continue\n",
    "        else:\n",
    "            state_action = []\n",
    "            for element in key:\n",
    "                for val in element:\n",
    "                    state_action.append(val)\n",
    "            Q = model.predict(np.array([np.array(state_action)]))\n",
    "            Qvalues[key] = Q[0]\n",
    "        print(i,'/',len(Qvalues),' Qvalues predicted')\n",
    "        i += 1\n",
    "    return Qvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot history function\n",
    "def plot_history(history):\n",
    "    # acc = history.history['acc']\n",
    "    # val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(loss) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    #plt.subplot(1, 2, 1)\n",
    "    # plt.plot(x, acc, 'b', label='Training acc')\n",
    "    # plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation loss')\n",
    "    # plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# early stopping is used to prevent overtraining -> we will stop the training \"early\" if it has reached maximum accuracy\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto',restore_best_weights=True)\n",
    "callbacks_list = [early_stopping]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_NN(X,Y):\n",
    "    # specify the input and output dimensions of the neural network\n",
    "    input_dim = 5\n",
    "    output_dim = 1\n",
    "    \n",
    "    batch_size = 0\n",
    "    if len(X) < 2500:\n",
    "        batch_size = 8\n",
    "    elif len(X) < 5000:\n",
    "        batch_size = 16\n",
    "    else:\n",
    "        batch_size = 32\n",
    "\n",
    "    model = Sequential()\n",
    "    # input layer\n",
    "    model.add(Dense(3000, input_dim=input_dim, activation = 'relu'))\n",
    "   # hidden layers\n",
    "    model.add(Dense(500, activation = 'relu'))\n",
    "    model.add(Dense(100, activation = 'relu'))\n",
    "    model.add(Dense(10, activation = 'relu'))\n",
    "    # output layer\n",
    "    model.add(Dense(output_dim, activation = 'linear'))\n",
    "    # Compile the architecture and view summary\n",
    "    model.compile(loss='mse',optimizer='adam')\n",
    "    history = model.fit(X, Y,\n",
    "                        epochs=20,\n",
    "                        verbose=1,\n",
    "                        validation_data = (X, Y),\n",
    "                        callbacks=callbacks_list,\n",
    "                        batch_size=batch_size)\n",
    "    print(model.summary())\n",
    "    \n",
    "    # print the loss of the model on training and validation set  \n",
    "    loss = model.evaluate(X, Y, verbose=False)\n",
    "    print(\"Training Loss: {:.4f}\".format(loss))\n",
    "    \n",
    "    # plot history\n",
    "    %matplotlib inline\n",
    "    plot_history(history)\n",
    "    \n",
    "    #sanity check\n",
    "    data = {'y_real':Y[0:50],'y_pred':(model.predict(X[0:50])).reshape(1,-1)[0]}\n",
    "    print(pd.DataFrame(data))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def periodic_test(iterations=150, angle_range=(-pi/8,pi/8,pi/32,pi/8,-pi/8,-pi/32,-pi/8,pi/8), angle_interval=pi/32, period=3, method='RF'):\n",
    "    \"\"\"\n",
    "    :param angle_range: (theta_lower,theta_upper,a1_lower,a1_upper,a2_lower,a2_upper,a_lower,a_upper)\n",
    "    \"\"\"\n",
    "    # learn Q values\n",
    "    print('Begin Function Approximation in period ', 1)\n",
    "    Qvalues, states, actions, _ = Qlearner(num_robots=iterations,\n",
    "            alpha=0.5,\n",
    "            gamma=0.9,\n",
    "            epsilon=0.5,\n",
    "            angle_range = angle_range,\n",
    "            angle_interval=angle_interval,\n",
    "            graphs=False,\n",
    "            live_graphs=False)\n",
    "    for i in range(period-1):\n",
    "        print('Begin Function Approximation in period ', i+2)\n",
    "        X, Y = get_training_inputs(Qvalues)\n",
    "        if method == 'RF':\n",
    "            # Random Forest\n",
    "            rf = RandomForestRegressor(n_estimators = 100)\n",
    "            rf.fit(X, Y)\n",
    "            # Calculate MAE\n",
    "            predictions = rf.predict(X)\n",
    "            errors = abs(predictions - Y)\n",
    "            print('Mean Absolute Error:', round(np.mean(errors), 2))\n",
    "            print('Making Q value predictions...')\n",
    "            Qvalues = fill_Q(Qvalues=Qvalues, model=rf)\n",
    "            print('Q value predictions done')\n",
    "        else:\n",
    "            #NN\n",
    "            NN = train_NN(X,Y)\n",
    "            print('Making Q value predictions...')\n",
    "            Qvalues = fill_Q(Qvalues=Qvalues, model=NN)\n",
    "            print('Q value predictions done')\n",
    "            \n",
    "        Qvalues, states, actions, _ = Qlearner(num_robots=iterations,\n",
    "            alpha=0.5,\n",
    "            gamma=0.9,\n",
    "            epsilon=0.5,\n",
    "            angle_range = angle_range,\n",
    "            angle_interval=angle_interval,\n",
    "            graphs=False,\n",
    "            live_graphs=False,\n",
    "            Qvalues=Qvalues)\n",
    "    \n",
    "    # extract policy\n",
    "    policy = extract_policy(model=None, states=states, actions=actions, Qvalues=Qvalues)\n",
    "    robot = ThreeLinkRobot(x=0,\n",
    "                       y=0,\n",
    "                       theta=pi/32,\n",
    "                       a1=pi/16,\n",
    "                       a2=-pi/16,\n",
    "                       link_length=2,\n",
    "                       t_interval=0.001,\n",
    "                       a_interval=angle_interval)\n",
    "    %matplotlib inline\n",
    "    x_displacement = test_policy(robot=robot, policy=policy, timestep=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "periodic_test(iterations=150, angle_range=(-pi/4,pi/4,pi/32,pi/4,-pi/4,-pi/32,-pi/8,pi/8), angle_interval=pi/32,period=2,method='RF')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
